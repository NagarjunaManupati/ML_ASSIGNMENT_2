{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Assignment 2 - Model Training\n",
    "## Complete Pipeline for Classification Models\n",
    "\n",
    "This notebook demonstrates the complete workflow for training and evaluating 6 classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, precision_score, \n",
    "    recall_score, f1_score, matthews_corrcoef,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Update these paths according to your dataset\n",
    "DATA_PATH = 'your_dataset.csv'  # Update this\n",
    "TARGET_COLUMN = 'target'  # Update this with your target column name\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"Number of Features: {df.shape[1] - 1}\")\n",
    "print(f\"Number of Instances: {df.shape[0]}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset information\n",
    "print(\"Dataset Info:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(f\"\\nTarget Distribution:\")\n",
    "print(df[TARGET_COLUMN].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop(columns=[TARGET_COLUMN])\n",
    "y = df[TARGET_COLUMN]\n",
    "\n",
    "# Encode target if categorical\n",
    "label_encoder = LabelEncoder()\n",
    "if y.dtype == 'object':\n",
    "    y = label_encoder.fit_transform(y)\n",
    "    print(f\"Target encoded. Classes: {label_encoder.classes_}\")\n",
    "\n",
    "# Handle categorical features\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "if len(categorical_cols) > 0:\n",
    "    print(f\"Encoding categorical features: {list(categorical_cols)}\")\n",
    "    X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "print(f\"\\nFinal feature shape: {X.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set size: {X_train_scaled.shape[0]}\")\n",
    "print(f\"Test set size: {X_test_scaled.shape[0]}\")\n",
    "print(f\"Number of features: {X_train_scaled.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all 6 models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'kNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss', use_label_encoder=False)\n",
    "}\n",
    "\n",
    "print(f\"Initialized {len(models)} models:\")\n",
    "for name in models.keys():\n",
    "    print(f\"  ✓ {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train and Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate all metrics\n",
    "def calculate_metrics(y_true, y_pred, y_pred_proba=None):\n",
    "    metrics = {}\n",
    "    \n",
    "    metrics['Accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    metrics['Precision'] = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    metrics['Recall'] = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    metrics['F1'] = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    metrics['MCC'] = matthews_corrcoef(y_true, y_pred)\n",
    "    \n",
    "    # AUC Score\n",
    "    try:\n",
    "        if y_pred_proba is not None:\n",
    "            if len(np.unique(y_true)) == 2:\n",
    "                metrics['AUC'] = roc_auc_score(y_true, y_pred_proba[:, 1])\n",
    "            else:\n",
    "                metrics['AUC'] = roc_auc_score(y_true, y_pred_proba, \n",
    "                                               multi_class='ovr', average='weighted')\n",
    "        else:\n",
    "            metrics['AUC'] = 0.0\n",
    "    except:\n",
    "        metrics['AUC'] = 0.0\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate all models\n",
    "results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Get probabilities\n",
    "    try:\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)\n",
    "    except:\n",
    "        y_pred_proba = None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = calculate_metrics(y_test, y_pred, y_pred_proba)\n",
    "    \n",
    "    # Store results\n",
    "    results[model_name] = {\n",
    "        'model': model,\n",
    "        'metrics': metrics,\n",
    "        'predictions': y_pred,\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "    print(f\"✓ {model_name} completed\")\n",
    "    print(f\"  Accuracy: {metrics['Accuracy']:.4f}\")\n",
    "    print(f\"  F1 Score: {metrics['F1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "results_data = []\n",
    "for model_name, result in results.items():\n",
    "    metrics = result['metrics']\n",
    "    results_data.append({\n",
    "        'ML Model Name': model_name,\n",
    "        'Accuracy': f\"{metrics['Accuracy']:.4f}\",\n",
    "        'AUC': f\"{metrics['AUC']:.4f}\",\n",
    "        'Precision': f\"{metrics['Precision']:.4f}\",\n",
    "        'Recall': f\"{metrics['Recall']:.4f}\",\n",
    "        'F1': f\"{metrics['F1']:.4f}\",\n",
    "        'MCC': f\"{metrics['MCC']:.4f}\"\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"MODEL COMPARISON TABLE\")\n",
    "print(\"=\"*100)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot metrics comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics_to_plot = ['Accuracy', 'AUC', 'Precision', 'Recall', 'F1', 'MCC']\n",
    "model_names = list(results.keys())\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    row = idx // 3\n",
    "    col = idx % 3\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    values = [results[name]['metrics'][metric] for name in model_names]\n",
    "    \n",
    "    bars = ax.bar(range(len(model_names)), values, color='steelblue', alpha=0.8)\n",
    "    ax.set_xlabel('Models', fontsize=10)\n",
    "    ax.set_ylabel(metric, fontsize=10)\n",
    "    ax.set_title(f'{metric} Comparison', fontsize=12, fontweight='bold')\n",
    "    ax.set_xticks(range(len(model_names)))\n",
    "    ax.set_xticklabels([name.replace(' ', '\\n') for name in model_names], \n",
    "                        rotation=0, ha='center', fontsize=8)\n",
    "    ax.set_ylim(0, 1.0)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, val in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{val:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Visualization saved as 'model_comparison.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Confusion Matrices for All Models', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, (model_name, result) in enumerate(results.items()):\n",
    "    row = idx // 3\n",
    "    col = idx % 3\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    cm = result['confusion_matrix']\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, cbar=False)\n",
    "    ax.set_title(model_name, fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Predicted', fontsize=10)\n",
    "    ax.set_ylabel('Actual', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Confusion matrices saved as 'confusion_matrices.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Models and Preprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs('model', exist_ok=True)\n",
    "\n",
    "# Save all models\n",
    "print(\"Saving models...\")\n",
    "for model_name, result in results.items():\n",
    "    model_filename = f\"model/{model_name.replace(' ', '_').lower()}_model.pkl\"\n",
    "    joblib.dump(result['model'], model_filename)\n",
    "    print(f\"✓ Saved {model_name} to {model_filename}\")\n",
    "\n",
    "# Save scaler and label encoder\n",
    "joblib.dump(scaler, 'model/scaler.pkl')\n",
    "joblib.dump(label_encoder, 'model/label_encoder.pkl')\n",
    "\n",
    "print(\"✓ Saved scaler and label encoder\")\n",
    "print(\"\\n✓ All models saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Results for README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "results_df.to_csv('model_results.csv', index=False)\n",
    "print(\"✓ Results exported to 'model_results.csv'\")\n",
    "\n",
    "# Print markdown table for README\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"COPY THIS TABLE TO YOUR README.md:\")\n",
    "print(\"=\"*100)\n",
    "print(\"\\n| ML Model Name | Accuracy | AUC | Precision | Recall | F1 | MCC |\")\n",
    "print(\"|--------------|----------|-----|-----------|--------|-------|-----|\")\n",
    "for _, row in results_df.iterrows():\n",
    "    print(f\"| {row['ML Model Name']} | {row['Accuracy']} | {row['AUC']} | {row['Precision']} | {row['Recall']} | {row['F1']} | {row['MCC']} |\")\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best model\n",
    "best_model_name = max(results.items(), key=lambda x: x[1]['metrics']['Accuracy'])[0]\n",
    "best_accuracy = results[best_model_name]['metrics']['Accuracy']\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\nBest Performing Model: {best_model_name}\")\n",
    "print(f\"Best Accuracy: {best_accuracy:.4f}\")\n",
    "print(f\"\\nAll models trained and saved successfully!\")\n",
    "print(f\"Total models: {len(models)}\")\n",
    "print(f\"\\nNext Steps:\")\n",
    "print(\"1. Update README.md with the results table\")\n",
    "print(\"2. Add your observations for each model\")\n",
    "print(\"3. Run the Streamlit app: streamlit run app.py\")\n",
    "print(\"4. Deploy to Streamlit Cloud\")\n",
    "print(\"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
